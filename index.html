<!DOCTYPE html>
<html>
<head>
  <link rel="shortcut icon" href="https://creadn.com.co/assets/img/icono.png">
  <meta charset="UTF-8">
  <title>Identificación de Emociones</title>
  <style>
    body {
      font-family: Arial, sans-serif;
      text-align: center;
      margin-top: 50px;
    }

    h1 {
      font-size: 24px;
    }

    button {
      padding: 10px 20px;
      font-size: 18px;
      margin: 10px;
    }

    #emotion-result {
      font-size: 18px;
      font-weight: bold;
    }
  </style>
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@3.9.0/dist/tf.min.js"></script>
  <script>
    // Cargar el modelo y definir las emociones disponibles
    async function loadModel() {
      const model = await tf.loadLayersModel('tfjs_model/model.json');
      return model;
    }

    const emotions = ['Agradecimiento', 'Ansiedad', 'Curiosidad', 'Expectativa', 'Felicidad', 'Seguridad', 'Tranquilidad'];

    // Variables para el audio
    let audioChunks = [];
    let stream;

    // Función para iniciar la grabación
    function startRecording() {
      navigator.mediaDevices.getUserMedia({ audio: true })
        .then(function (stream) {
          audioChunks = [];
          const mediaRecorder = new MediaRecorder(stream);

          mediaRecorder.addEventListener('dataavailable', function (event) {
            audioChunks.push(event.data);
          });

          mediaRecorder.start();
        })
        .catch(function (error) {
          console.error('Error al acceder al micrófono:', error);
        });
    }

    // Función para detener la grabación y predecir la emoción
    function stopRecording() {
      if (stream) {
        const tracks = stream.getTracks();
        tracks.forEach(function (track) {
          track.stop();
        });
        stream = null;

        const mediaRecorder = audioChunks.length > 0 ? audioChunks[0].recorder : null;
        if (mediaRecorder && mediaRecorder.state !== 'inactive') {
          mediaRecorder.addEventListener('stop', async function () {
            const audioBlob = new Blob(audioChunks, { type: 'audio/wav' });
            const audioUrl = URL.createObjectURL(audioBlob);
            await predictEmotion(audioUrl);
          });
          mediaRecorder.stop();
        }
      }
    }

    // Función para preprocesar el audio
    function preprocessAudio(audioUrl) {
      return new Promise(function (resolve, reject) {
        const audio = new Audio();
        audio.src = audioUrl;

        audio.addEventListener('loadedmetadata', function () {
          const audioContext = new AudioContext();
          const source = audioContext.createMediaElementSource(audio);
          const scriptProcessor = audioContext.createScriptProcessor(4096, 1, 1);

          const audioData = [];

          scriptProcessor.addEventListener('audioprocess', function (event) {
            const channelData = event.inputBuffer.getChannelData(0);

            for (let i = 0;          channelData.length; i++) {
              audioData.push(channelData[i]);
            }
          });

          scriptProcessor.addEventListener('complete', function () {
            resolve(audioData);
          });

          source.connect(scriptProcessor);
          scriptProcessor.connect(audioContext.destination);
          audio.play();
        });

        audio.addEventListener('error', function (error) {
          reject(error);
        });
      });
    }

    // Función para mostrar la emoción identificada
    function showEmotionResult(emotion) {
      const resultElement = document.getElementById('emotion-result');
      resultElement.textContent = `Emoción identificada: ${emotion}`;
    }

    // Función para predecir la emoción y mostrar los resultados
    async function predictEmotion(audioUrl) {
      try {
        const model = await loadModel();
        const audioData = await preprocessAudio(audioUrl);
        const audioTensor = tf.tensor(audioData);
        const audioTensorNormalized = audioTensor.div(tf.max(tf.abs(audioTensor)));
        const audioTensorReshaped = audioTensorNormalized.reshape([1, audioTensorNormalized.shape[0], 1]);
        const audioTensorExpanded = audioTensorReshaped.expandDims(-1);
        const predictions = model.predict(audioTensorExpanded);
        const emotionIndex = predictions.argMax(1).dataSync()[0];
        const emotion = emotions[emotionIndex];

        // Mostrar la emoción identificada
        showEmotionResult(emotion);
      } catch (error) {
        console.error('Error al predecir la emoción:', error);
      }
    }
  </script>
</head>
<body>
  <h1>Identificación de Emociones</h1>
  <button onclick="startRecording()">Grabar</button>
  <button onclick="stopRecording()">Detener</button>
  <p id="emotion-result"></p>
</body>
</html>
